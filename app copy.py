# KBS êµ­ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ëŒ€ì‹œë³´ë“œ
# ë‚ ì§œë³„ í¬ë¡¤ë§ + í‚¤ì›Œë“œ ë¶„ì„ + íŠ¸ë Œë“œ ì‹œê°í™” + AI ì¸ì‚¬ì´íŠ¸

import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
from openai import OpenAI
import time
from datetime import datetime
import io
import os
from dotenv import load_dotenv
from collections import Counter
import re
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# íŽ˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="KBS êµ­ì œ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ",
    page_icon="ðŸ“Š",
    layout="wide"
)

# ì œëª©
st.title("ðŸ“Š KBS êµ­ì œ ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.markdown("ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ + AI ë¶„ì„ + í‚¤ì›Œë“œ íŠ¸ë Œë“œ")
st.markdown("---")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ í¬ë¡¤ë§ ì„¤ì •")

    # API í‚¤ (.envì—ì„œ ìžë™ ë¡œë“œ)
    env_api_key = os.getenv("OPENAI_API_KEY", "")

    # UIì—ì„œ í‚¤ ìž…ë ¥ (ì„ íƒì‚¬í•­)
    user_input_key = st.text_input(
        "OpenAI API í‚¤ (ì„ íƒ)",
        type="password",
        value="",
        placeholder="ë¹„ì›Œë‘ë©´ .env íŒŒì¼ì˜ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤",
        help=".env íŒŒì¼ì— í‚¤ê°€ ìžˆìœ¼ë©´ ìž…ë ¥í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤"
    )

    # ì‚¬ìš©ìž ìž…ë ¥ì´ ìžˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ .env í‚¤ ì‚¬ìš©
    api_key = user_input_key if user_input_key else env_api_key

    # API í‚¤ ìƒíƒœ í‘œì‹œ
    if api_key:
        st.success("âœ… API í‚¤ ì„¤ì •ë¨")
    else:
        st.warning("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    st.markdown("---")

    # ë‚ ì§œ ë²”ìœ„
    st.subheader("ðŸ“… í¬ë¡¤ë§ ë‚ ì§œ")
    num_days = st.slider("í¬ë¡¤ë§í•  ë‚ ì§œ ìˆ˜", 1, 30, 3)

    st.markdown("---")

    # íŽ˜ì´ì§€ ìˆ˜
    st.subheader("ðŸ“„ íŽ˜ì´ì§€ ì„¤ì •")
    pages_per_day = st.slider("ë‚ ì§œë‹¹ íŽ˜ì´ì§€ ìˆ˜", 1, 10, 2)

    st.markdown("---")
    start_crawling = st.button("ðŸš€ í¬ë¡¤ë§ ì‹œìž‘", type="primary", use_container_width=True)

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (í•œê¸€)
def extract_keywords(text):
    """í•œê¸€ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬ ì¤‘ì‹¬)"""
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {'ìžˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•Šë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë§Žë‹¤',
                 'í¬ë‹¤', 'ìž‘ë‹¤', 'ë†’ë‹¤', 'ë‚®ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ìœ„í•´', 'í†µí•´',
                 'ëŒ€í•œ', 'ìžˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì´ë²ˆ', 'ì˜¬í•´', 'ì§€ë‚œ', 'ì˜¤ëŠ˜'}

    # 2ê¸€ìž ì´ìƒ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
    words = re.findall(r'[ê°€-íž£]{2,}', text)

    # ë¶ˆìš©ì–´ ì œê±° ë° ì¹´ìš´íŠ¸
    filtered_words = [w for w in words if w not in stopwords]

    return filtered_words

# OpenAI ìš”ì•½ í•¨ìˆ˜
def summarize_news(content, client):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‰´ìŠ¤ë¥¼ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ 3ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ 3ì¤„ë¡œ ìš”ì•½:\n\n{content}"}
            ],
            max_tokens=200,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"

# AI ì¸ì‚¬ì´íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_insights(df, top_keywords, client):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    try:
        # ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ ìƒ˜í”Œ
        sample_titles = "\n".join(df['ë‰´ìŠ¤ ì œëª©'].head(10).tolist())
        keyword_str = ", ".join([f"{k}({v}ê±´)" for k, v in top_keywords[:10]])

        prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ êµ­ì œ ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ê²°ê³¼ìž…ë‹ˆë‹¤:

**ìˆ˜ì§‘ ê¸°ê°„**: {df['ê¸°ê³  ë‚ ì§œ'].min()} ~ {df['ê¸°ê³  ë‚ ì§œ'].max()}
**ì´ ë‰´ìŠ¤ ìˆ˜**: {len(df)}ê°œ
**ì£¼ìš” í‚¤ì›Œë“œ**: {keyword_str}

**ì£¼ìš” ë‰´ìŠ¤ ì œëª©ë“¤**:
{sample_titles}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê´€ì ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. ðŸ”¥ í˜„ìž¬ ê°€ìž¥ í•«í•œ êµ­ì œ ì´ìŠˆ (3ê°œ)
2. ðŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ (ì–´ë–¤ ì£¼ì œê°€ ë¶€ìƒí•˜ê³  ìžˆëŠ”ì§€)
3. ðŸ’¡ ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸ (ìˆ¨ê²¨ì§„ íŒ¨í„´ì´ë‚˜ ì—°ê´€ì„±)

ê° í•­ëª©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•´ì£¼ì„¸ìš”.
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ êµ­ì œ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.8
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news(num_days, pages_per_day, api_key, progress_bar, status_text):
    client = OpenAI(api_key=api_key)

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")

    driver = webdriver.Chrome(options=chrome_options)
    data = []
    base_url = "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0006&ref=pSiteMap"

    total_steps = num_days * pages_per_day
    current_step = 0

    try:
        status_text.text("ðŸŒ KBS êµ­ì œ ë‰´ìŠ¤ ì ‘ì† ì¤‘...")
        driver.get(base_url)
        time.sleep(3)

        for day_num in range(num_days):
            try:
                current_date = driver.find_element(By.CSS_SELECTOR, ".datepicker-label .date").text
            except:
                current_date = datetime.now().strftime("%Y.%m.%d")

            status_text.text(f"ðŸ“… [{day_num + 1}/{num_days}ì¼] {current_date} í¬ë¡¤ë§ ì¤‘...")

            for page_num in range(1, pages_per_day + 1):
                current_step += 1
                progress_bar.progress(current_step / total_steps)

                time.sleep(2)
                soup = BeautifulSoup(driver.page_source, "html.parser")
                news_items = soup.select(".box-contents.has-wrap .box-content")

                for item in news_items:
                    try:
                        link = item.get("href")
                        if not link:
                            continue

                        news_url = "https://news.kbs.co.kr" + link
                        title_elem = item.select_one(".title")
                        title = title_elem.text.strip() if title_elem else "ì œëª© ì—†ìŒ"

                        date_elem = item.select_one(".field-writer .date")
                        date = date_elem.text.strip() if date_elem else current_date

                        driver.execute_script(f"window.open('{news_url}', '_blank');")
                        driver.switch_to.window(driver.window_handles[1])
                        time.sleep(1)

                        detail_soup = BeautifulSoup(driver.page_source, "html.parser")
                        content_elem = detail_soup.select_one("#cont_newstext")
                        content = content_elem.text.strip() if content_elem else "ë‚´ìš© ì—†ìŒ"

                        summary = summarize_news(content, client)
                        data.append([date, title, content, summary])

                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
                        time.sleep(0.5)

                    except Exception as e:
                        if len(driver.window_handles) > 1:
                            driver.close()
                            driver.switch_to.window(driver.window_handles[0])
                        continue

                if page_num < pages_per_day:
                    try:
                        next_button = driver.find_element(By.CSS_SELECTOR, f"#page{page_num + 1}")
                        driver.execute_script("arguments[0].click();", next_button)
                        time.sleep(2)
                    except:
                        break

            if day_num < num_days - 1:
                try:
                    prev_button = driver.find_element(By.CSS_SELECTOR, ".previous-button")
                    driver.execute_script("arguments[0].click();", prev_button)
                    time.sleep(3)
                except:
                    break

    finally:
        driver.quit()

    return data

# í¬ë¡¤ë§ ì‹¤í–‰
if start_crawling:
    if not api_key:
        st.error("âŒ OpenAI API í‚¤ë¥¼ ìž…ë ¥í•˜ì„¸ìš”!")
    else:
        progress_bar = st.progress(0)
        status_text = st.empty()

        with st.spinner("í¬ë¡¤ë§ ì¤‘..."):
            data = crawl_news(num_days, pages_per_day, api_key, progress_bar, status_text)

        progress_bar.progress(1.0)
        status_text.text("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")

        if data:
            df = pd.DataFrame(data, columns=["ê¸°ê³  ë‚ ì§œ", "ë‰´ìŠ¤ ì œëª©", "ë‰´ìŠ¤ ë‚´ìš©", "3ì¤„ ìš”ì•½"])
            st.session_state['df'] = df
            st.success(f"âœ… ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ëŒ€ì‹œë³´ë“œ í‘œì‹œ
if 'df' in st.session_state:
    df = st.session_state['df']
    client = OpenAI(api_key=api_key)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    all_text = " ".join(df['ë‰´ìŠ¤ ì œëª©'] + " " + df['ë‰´ìŠ¤ ë‚´ìš©'])
    keywords = extract_keywords(all_text)
    keyword_counts = Counter(keywords)
    top_keywords = keyword_counts.most_common(20)

    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š ëŒ€ì‹œë³´ë“œ", "ðŸ” í‚¤ì›Œë“œ ë¶„ì„", "ðŸ’¡ AI ì¸ì‚¬ì´íŠ¸", "ðŸ“¥ ë°ì´í„°"])

    with tab1:
        st.header("ðŸ“Š ë‰´ìŠ¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

        # í†µê³„ ì¹´ë“œ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ ë‰´ìŠ¤", f"{len(df)}ê°œ")
        with col2:
            st.metric("ìˆ˜ì§‘ ë‚ ì§œ", f"{df['ê¸°ê³  ë‚ ì§œ'].nunique()}ì¼")
        with col3:
            st.metric("í‰ê·  ê¸¸ì´", f"{df['ë‰´ìŠ¤ ë‚´ìš©'].str.len().mean():.0f}ìž")
        with col4:
            st.metric("ì£¼ìš” í‚¤ì›Œë“œ", f"{len(top_keywords)}ê°œ")

        st.markdown("---")

        # ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ì°¨íŠ¸
        st.subheader("ðŸ“ˆ ë‚ ì§œë³„ ë‰´ìŠ¤ íŠ¸ë Œë“œ")
        date_counts = df['ê¸°ê³  ë‚ ì§œ'].value_counts().sort_index()
        fig_timeline = px.line(
            x=date_counts.index,
            y=date_counts.values,
            labels={'x': 'ë‚ ì§œ', 'y': 'ë‰´ìŠ¤ ê°œìˆ˜'},
            title="ë‚ ì§œë³„ ë‰´ìŠ¤ ë°œí–‰ ì¶”ì´"
        )
        fig_timeline.update_traces(mode='lines+markers', line_color='#FF6B6B')
        st.plotly_chart(fig_timeline, use_container_width=True)

        st.markdown("---")

        # Top 10 í‚¤ì›Œë“œ ë°” ì°¨íŠ¸
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ðŸ” Top 10 í‚¤ì›Œë“œ")
            top10_keywords = top_keywords[:10]
            fig_bar = px.bar(
                x=[k[1] for k in top10_keywords],
                y=[k[0] for k in top10_keywords],
                orientation='h',
                labels={'x': 'ë¹ˆë„', 'y': 'í‚¤ì›Œë“œ'},
                title="ê°€ìž¥ ë§Žì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ"
            )
            fig_bar.update_traces(marker_color='#4ECDC4')
            st.plotly_chart(fig_bar, use_container_width=True)

        with col2:
            st.subheader("â˜ï¸ ì›Œë“œ í´ë¼ìš°ë“œ")
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wordcloud = WordCloud(
                font_path='C:\\Windows\\Fonts\\malgun.ttf',  # í•œê¸€ í°íŠ¸
                width=800,
                height=400,
                background_color='white',
                colormap='viridis'
            ).generate_from_frequencies(dict(top_keywords))

            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')
            st.pyplot(fig)

    with tab2:
        st.header("ðŸ” í‚¤ì›Œë“œ ìƒì„¸ ë¶„ì„")

        # í‚¤ì›Œë“œ ê²€ìƒ‰
        search_keyword = st.text_input("ðŸ”Ž í‚¤ì›Œë“œ ê²€ìƒ‰", placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ìž…ë ¥í•˜ì„¸ìš”")

        if search_keyword:
            filtered = df[df['ë‰´ìŠ¤ ì œëª©'].str.contains(search_keyword, case=False, na=False) |
                         df['ë‰´ìŠ¤ ë‚´ìš©'].str.contains(search_keyword, case=False, na=False)]
            st.info(f"'{search_keyword}' ê´€ë ¨ ë‰´ìŠ¤: {len(filtered)}ê°œ")
            st.dataframe(filtered[['ê¸°ê³  ë‚ ì§œ', 'ë‰´ìŠ¤ ì œëª©', '3ì¤„ ìš”ì•½']], use_container_width=True)

        st.markdown("---")

        # ì „ì²´ í‚¤ì›Œë“œ í…Œì´ë¸”
        st.subheader("ðŸ“‹ ì „ì²´ í‚¤ì›Œë“œ ìˆœìœ„")
        keyword_df = pd.DataFrame(top_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        keyword_df['ìˆœìœ„'] = range(1, len(keyword_df) + 1)
        st.dataframe(keyword_df[['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'ë¹ˆë„']], use_container_width=True, height=400)

    with tab3:
        st.header("ðŸ’¡ AI ì¸ì‚¬ì´íŠ¸")

        if st.button("ðŸ¤– AI ì¸ì‚¬ì´íŠ¸ ìƒì„±", type="primary"):
            with st.spinner("AIê°€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ìžˆìŠµë‹ˆë‹¤..."):
                insights = generate_insights(df, top_keywords, client)
                st.session_state['insights'] = insights

        if 'insights' in st.session_state:
            st.markdown(st.session_state['insights'])

    with tab4:
        st.header("ðŸ“¥ ë°ì´í„° ë° ë‹¤ìš´ë¡œë“œ")

        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ðŸ“‹ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„°")
        st.dataframe(df, use_container_width=True, height=400)

        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='êµ­ì œë‰´ìŠ¤')
            keyword_df.to_excel(writer, index=False, sheet_name='í‚¤ì›Œë“œ')
        excel_data = output.getvalue()

        st.download_button(
            label="ðŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ë‰´ìŠ¤ + í‚¤ì›Œë“œ)",
            data=excel_data,
            file_name=f"êµ­ì œë‰´ìŠ¤_ë¶„ì„_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )

else:
    # ì´ˆê¸° í™”ë©´
    st.info("ðŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ 'í¬ë¡¤ë§ ì‹œìž‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")

    with st.expander("ðŸ“– ì‚¬ìš© ê°€ì´ë“œ"):
        st.markdown("""
        ### ðŸš€ ì‚¬ìš© ë°©ë²•

        1. **API í‚¤**: .env íŒŒì¼ì—ì„œ ìžë™ ë¡œë“œ
        2. **ë‚ ì§œ ì„¤ì •**: í¬ë¡¤ë§í•  ë‚ ì§œ ìˆ˜ ì„ íƒ
        3. **íŽ˜ì´ì§€ ì„¤ì •**: ë‚ ì§œë‹¹ íŽ˜ì´ì§€ ìˆ˜ ì„ íƒ
        4. **í¬ë¡¤ë§ ì‹œìž‘**: ë²„íŠ¼ í´ë¦­
        5. **ëŒ€ì‹œë³´ë“œ**: 4ê°œ íƒ­ì—ì„œ ë¶„ì„ ê²°ê³¼ í™•ì¸

        ### ðŸ“Š ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥

        - **ðŸ“Š ëŒ€ì‹œë³´ë“œ**: í†µê³„, íŠ¸ë Œë“œ, ì›Œë“œí´ë¼ìš°ë“œ
        - **ðŸ” í‚¤ì›Œë“œ ë¶„ì„**: í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ìˆœìœ„
        - **ðŸ’¡ AI ì¸ì‚¬ì´íŠ¸**: GPT ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„
        - **ðŸ“¥ ë°ì´í„°**: ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        """)
